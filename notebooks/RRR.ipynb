{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d708d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# External modules\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import importlib\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "sys.path.append(os.path.abspath('../utils/'))\n",
    "\n",
    "# Local modules\n",
    "import get_oms_data\n",
    "from get_oms_data import get_oms_api, get_oms_data, get_oms_response_attribute\n",
    "\n",
    "import json_utils as jsonu\n",
    "import plot_utils as pu\n",
    "\n",
    "import mplhep as hep\n",
    "hep.style.use(\"CMS\")\n",
    "\n",
    "import json_utils\n",
    "from refruns_utils import get_reference_run as RRfetch\n",
    "import refrank_utils as rrr\n",
    "import oms_utils as omsu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a0c37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(get_oms_data)\n",
    "importlib.reload(rrr)\n",
    "# importlib.reload(omsu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20465e66",
   "metadata": {},
   "source": [
    "# Getting the Data\n",
    "We start by instantiating the OMS API. The OMS API will be used to get Run and LS level data and will be the main source of information for the reference run ranking (RRR) system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5291df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "omsapi = get_oms_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1f79f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the features that will be fetched from OMS\n",
    "# Run level attributes\n",
    "run_attribs = [\n",
    "    \"run_number\", #\n",
    "    \"init_lumi\", #\n",
    "    \"recorded_lumi\",\n",
    "    \"delivered_lumi\",\n",
    "    \"energy\",\n",
    "    \"end_lumi\", #\n",
    "    \"hlt_physics_rate\",\n",
    "    \"fill_number\", # Neccesary for filtering NaNs\n",
    "    \"initial_prescale_index\",\n",
    "    \"last_lumisection_number\",\n",
    "    \"l1_rate\",\n",
    "    \"hlt_physics_counter\",\n",
    "    \"l1_hlt_mode\",\n",
    "    \"duration\"\n",
    "]\n",
    "\n",
    "# Lumisecion (LS) level attributes\n",
    "ls_attribs = [\n",
    "    \"fill_number\", # Neccesary for filtering NaNs\n",
    "    \"run_number\",\n",
    "    \"lumisection_number\",\n",
    "    \"pileup\",\n",
    "    \"delivered_lumi\",\n",
    "    \"recorded_lumi\",\n",
    "    \"init_lumi\",\n",
    "    \"end_lumi\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8783f777",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using the API, you can check all of the available features\n",
    "run_query = rrr.makeDF(get_oms_data(omsapi, \"runs\", 316201, limit_entries=1))\n",
    "print(\"All available run level features:\\n\")\n",
    "run_query.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdad12b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using the API, you can check all of the available features\n",
    "ls_query = rrr.makeDF(get_oms_data(omsapi, \"lumisections\", 316201, limit_entries=1))\n",
    "print(\"All available LS level features:\\n\")\n",
    "ls_query.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d145f18",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb1cb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run we wish to certify and for which we will find a correspond reference run\n",
    "# newest_run = 326201\n",
    "# oldest_run = 309000\n",
    "\n",
    "# All 2018 runs\n",
    "# newest_run = 327802\n",
    "# oldest_run = 308241\n",
    "\n",
    "# Almost all 2018 runs\n",
    "newest_run = 326201\n",
    "oldest_run = 308800 # Oldest run OMS is allowing me to get\n",
    "\n",
    "# Range of runs of interest\n",
    "run_range = (oldest_run, newest_run)\n",
    "print(\"Run range: {}\".format(run_range))\n",
    "print(\"Potential total number of runs to load: {}\".format(newest_run - oldest_run))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13babf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filters to apply to data fetched from OMS\n",
    "run_filters = [\n",
    "    {\"attribute_name\": \"l1_hlt_mode\", \"value\": \"collisions\", \"operator\": \"LIKE\"} # Only collision runs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bcdc7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_df, ls_df = omsu.get_runs_lss(run_range, omsapi, run_attribs, ls_attribs, run_filters=run_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f6251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save to parquet\n",
    "# run_df.to_parquet('/eos/user/r/rcruzcan/SWAN_projects/RefRunRank/data/omsrundata_2018.parquet')\n",
    "# ls_df.to_parquet('/eos/user/r/rcruzcan/SWAN_projects/RefRunRank/data/omslsdata_2018.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d214fec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Load from parquet\n",
    "run_df = pd.read_parquet('/eos/user/r/rcruzcan/SWAN_projects/RefRunRank/data/omsrundata_2018.parquet')\n",
    "ls_df = pd.read_parquet('/eos/user/r/rcruzcan/SWAN_projects/RefRunRank/data/omslsdata_2018.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2730c5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check of the loaded runs and LSs\n",
    "print(\"Number of runs loaded from OMS: {}\".format(len(run_df)))\n",
    "print(\"Number of lumisections loaded from OMS: {}\".format(len(ls_df)))\n",
    "print(\"Number of runs in the LS df: {}\".format(len(ls_df[\"run_number\"].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b9ca5a",
   "metadata": {},
   "source": [
    "<font color='green'> Why does `ls_df` have so many more runs that `run_df`? Reason: `run_df` has been pre-filtered, `ls_df` hasn't. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8c9416",
   "metadata": {},
   "source": [
    "## Filtering out NaN values\n",
    "\n",
    "\n",
    "Some of the rows has `fill_number = NaN`. This is problematic, so we will segment the rows into those that have this problem (`run_df_NaN`), and the rest (`run_df`). We also add `Fill Location` column and make the `run_number` column into an index column.\n",
    "\n",
    "<font color='red'> INVESTIGATE WHY THESE RUNS/LSs HAVE NaN VALUES. FIGURE WHERE THESE LSs TEND TO HAPPEN (START OR END OF THE RUN). </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b85fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out runs that have NaN in fill number\n",
    "run_df, run_df_NaN = rrr.has_fill(run_df)\n",
    "print(\n",
    "    '''\n",
    "    Runs kept in run dataframe: {}\n",
    "    Runs filtered out: {}\n",
    "    Lowest run # kept: {}\n",
    "    Highest run # kept: {}\n",
    "    '''.format(\n",
    "        len(run_df['run_number'].unique()), \n",
    "        len(run_df_NaN['run_number'].unique()),\n",
    "        run_df['run_number'].min(),\n",
    "        run_df['run_number'].max())\n",
    ")\n",
    "\n",
    "run_df = run_df.convert_dtypes()\n",
    "\n",
    "# run_df = rrr.add_loc_wrt_fill(run_df)\n",
    "run_df.set_index(['run_number'], inplace=True)\n",
    "run_df.sort_index(level=['run_number'], inplace=True)\n",
    "# run_df.reset_index(inplace=True)\n",
    "run_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de40d3bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filter out LSs that have NaN values\n",
    "ls_df, lsNaN_df = rrr.has_fill(ls_df)\n",
    "print(\n",
    "    '''\n",
    "    Number of runs in LSs df: {}\n",
    "    Runs filtered out: {}\n",
    "    Lowest run # kept: {}\n",
    "    Highest run # kept: {}\n",
    "    '''.format(\n",
    "        len(ls_df['run_number'].unique()),   \n",
    "        len(lsNaN_df['run_number'].unique()),\n",
    "        ls_df['run_number'].min(),\n",
    "        ls_df['run_number'].max()\n",
    "    )\n",
    ")\n",
    "\n",
    "ls_df = ls_df.convert_dtypes()\n",
    "\n",
    "# ls_df = rrr.add_loc_wrt_fill(ls_df)\n",
    "ls_df.set_index(['run_number', 'lumisection_number'], inplace=True)\n",
    "ls_df.sort_index(level=['run_number', 'lumisection_number'], inplace=True)\n",
    "# ls_df.reset_index(inplace=True)\n",
    "ls_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f08d8b",
   "metadata": {},
   "source": [
    "## Keeping only good runs\n",
    "Note: It might be neccesary to generate a new golden JSON in RR depending on the range of runs chosen for ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8925377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant golden jsons available in the json directory\n",
    "golden_files = [\n",
    "    \"json_GOLDEN_2017.json\", \n",
    "    \"json_GOLDEN_2018.json\",\n",
    "    \"json_GOLDEN_RRRdev.json\",\n",
    "    \"json_TRK_PromptReco_314324to316201.json\",\n",
    "    \"json_TRK_PromptReco_2018Runs.json\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dc3fa1",
   "metadata": {},
   "source": [
    "Generated golden JSON such as `json_GOLDEN_RRRdev.json` made in RR using input similar to:\n",
    "```json\n",
    "{\n",
    "  \"and\": [\n",
    "    {\">=\": [{\"var\": \"run.oms.run_number\"}, 315190]},\n",
    "    {\"<=\": [{\"var\": \"run.oms.run_number\"}, 316201]},\n",
    "    {\"==\": [{\"var\": \"lumisection.rr.tracker-pixel\"}, \"GOOD\"]},\n",
    "    {\"==\": [{\"var\": \"lumisection.rr.tracker-strip\"}, \"GOOD\"]},\n",
    "    {\"==\": [{\"var\": \"lumisection.rr.tracker-track\"}, \"GOOD\"]}\n",
    "  ]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c93c115",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_df.reset_index(inplace=True)\n",
    "ls_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bddcd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using golden json to know which LSs are bad and which ones are good\n",
    "ls_df['is_good'] = json_utils.injson(\n",
    "    np.array(ls_df['run_number']), \n",
    "    np.array(ls_df['lumisection_number']), \n",
    "    '/eos/user/r/rcruzcan/SWAN_projects/RefRunRank/jsons/'+golden_files[4]\n",
    ")\n",
    "\n",
    "# Computing \"run quality\" metric\n",
    "quality_score = ls_df.groupby(\"run_number\")[\"is_good\"].mean()\n",
    "\n",
    "# Identify runs with quality < 0.75 as bad runs\n",
    "bad_runs = quality_score[quality_score < 0.75].index.tolist()\n",
    "\n",
    "# Classifying run as good or bad depending on their quality score\n",
    "\n",
    "runbad_df = run_df[run_df['run_number'].isin(bad_runs)]\n",
    "run_df = run_df[~run_df['run_number'].isin(bad_runs)]\n",
    "\n",
    "lsbad_df = ls_df[ls_df['run_number'].isin(bad_runs)]\n",
    "ls_df = ls_df[~ls_df['run_number'].isin(bad_runs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415d2709",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    '''\n",
    "        Number of good collision LSs: {}\n",
    "        Number of bad collisions LSs: {}\n",
    "        Number of runs in good df: {}\n",
    "        Number of runs in bad df: {}\n",
    "    '''.format(len(ls_df), len(lsbad_df), len(run_df), len(runbad_df))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07700989",
   "metadata": {},
   "source": [
    "<font color='red'> Old approach. Compare which of the two works best </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de28c9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Dividing good runs/LSs from bad runs/LSs\n",
    "# good_mask = json_utils.injson(\n",
    "#     np.array(ls_df['run_number']), \n",
    "#     np.array(ls_df['lumisection_number']), \n",
    "#     '/eos/user/r/rcruzcan/SWAN_projects/RefRunRank/jsons/'+golden_files[4])\n",
    "\n",
    "# lsbad_df = ls_df[~good_mask]\n",
    "# ls_df = ls_df[good_mask]\n",
    "\n",
    "# runbad_df = run_df[~run_df['run_number'].isin(ls_df['run_number'].unique())]\n",
    "# run_df = run_df[run_df['run_number'].isin(ls_df['run_number'].unique())]\n",
    "\n",
    "# print(\n",
    "#     '''\n",
    "#         Number of good collision LSs: {}\n",
    "#         Number of bad collisions LSs: {}\n",
    "#         Number of runs in good df: {}\n",
    "#         Number of runs in bad df: {}\n",
    "#     '''.format(len(ls_df), len(lsbad_df), len(ls_df[\"run_number\"].unique()), len(lsbad_df[\"run_number\"].unique()))\n",
    "# )\n",
    "\n",
    "# print(\n",
    "#     '''\n",
    "#         Number of good collision runs: {}\n",
    "#         Number of bad collisions runs: {}\n",
    "#     '''.format(len(run_df), len(runbad_df))\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a803a2",
   "metadata": {},
   "source": [
    "## Keeping only `collision2018` runs\n",
    "(Note: The final filtering for the run type of interest is done here so that this notebook can be copied and re-used more easily for other types of runs like cosmics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67efab50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Types of runs in our data: \")\n",
    "print(run_df[\"l1_hlt_mode\"].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21be8955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, we are only interested in collision2018 runs, so we discard the rest.\n",
    "run_notcoll2018_df = run_df[run_df[\"l1_hlt_mode\"] != \"collisions2018\"]\n",
    "run_df = run_df[run_df[\"l1_hlt_mode\"] == \"collisions2018\"]\n",
    "\n",
    "coll_runs = np.array(run_df[\"run_number\"])\n",
    "ls_notcoll2018_df = ls_df[~ls_df[\"run_number\"].isin(coll_runs)]\n",
    "ls_df = ls_df[ls_df[\"run_number\"].isin(coll_runs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1697e134",
   "metadata": {},
   "source": [
    "## Keeping only runs that appear in both dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71557daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking which runs appear in one dataframe, but not the other, and dropping them from both dataframes.\n",
    "\n",
    "rundf_runs = np.array(run_df[\"run_number\"])\n",
    "lsdf_runs = np.array(ls_df[\"run_number\"])\n",
    "\n",
    "# Finding symmetric different\n",
    "missing_runs = np.setxor1d(rundf_runs, lsdf_runs)\n",
    "\n",
    "print(\n",
    "    '''\n",
    "    List of runs which are in one of the dataframes, but not the other (and viceversa): \\n    {}\n",
    "    '''.format(missing_runs)\n",
    ")\n",
    "\n",
    "# Removing runs that are not in both dataframes\n",
    "run_df = run_df[~run_df[\"run_number\"].isin(missing_runs)]\n",
    "ls_df = ls_df[~ls_df[\"run_number\"].isin(missing_runs)]\n",
    "\n",
    "# run_df = run_df[~run_df.index.isin(missing_runs)]\n",
    "# ls_df = ls_df[~ls_df.index.get_level_values(0).isin(missing_runs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066a38d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check that the run filtering step worked\n",
    "rundf_runs = np.array(run_df[\"run_number\"])\n",
    "lsdf_runs = np.array(ls_df[\"run_number\"])\n",
    "\n",
    "# Finding symmetric different\n",
    "missing_runs = np.setxor1d(rundf_runs, lsdf_runs)\n",
    "\n",
    "print(\n",
    "    '''\n",
    "    List of runs which are in one of the dataframes, but not the other (and viceversa): \\n    {}\n",
    "    '''.format(missing_runs)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc6f4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    '''\n",
    "    Number of runs in the run dataframe: {}\n",
    "    Number of runs in the lumisection dataframe: {}\n",
    "    Are the runs contained in these dataframes exactly the same?: {}\n",
    "    '''.format(\n",
    "        len(run_df),\n",
    "        len(ls_df[\"run_number\"].unique()),\n",
    "        np.array_equal(run_df[\"run_number\"], ls_df[\"run_number\"].unique())\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776281fd",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "\n",
    "We create some plots to better understand how this data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e2e1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lumi_info = ls_df[['run_number',\n",
    "                           'init_lumi', \n",
    "                           'end_lumi',\n",
    "                           'pileup',\n",
    "                           'recorded_lumi', \n",
    "                           'delivered_lumi',\n",
    "                           'lumisection_number',]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3920dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lumis(run_begin, run_end, lumi_info, run_labels=True):\n",
    "    \"\"\"\n",
    "    Plotting init_lumi, pileup, recorded_lumi and delivered_lumi for a couple of runs\n",
    "    \"\"\"\n",
    "\n",
    "    lumi_info['run_lumisection'] = list(zip(lumi_info['run_number'], lumi_info['lumisection_number']))\n",
    "\n",
    "    filtered_lumi_info = lumi_info[(lumi_info['run_number'] >= run_begin) & (lumi_info['run_number'] <= run_end)]\n",
    "\n",
    "    fig, ax = plt.subplots(dpi=200)\n",
    "    \n",
    "    if run_labels:\n",
    "        current_run = None\n",
    "        for idx, (run, lumisection) in enumerate(filtered_lumi_info['run_lumisection']):\n",
    "            if run != current_run:\n",
    "                ax.axvline(x=idx, color='gray', linestyle='--', linewidth=0.5)  # Add a vertical line\n",
    "\n",
    "                # Add a label for the run number\n",
    "                if current_run is not None:  # Skip label for the very first line\n",
    "                    label_x_position = idx - 1  # Adjust this to position the label\n",
    "                    ax.text(label_x_position, 0.95, str(current_run), transform=ax.get_xaxis_transform(), \n",
    "                            horizontalalignment='right', verticalalignment='top', fontsize=6, color='gray', rotation='vertical')\n",
    "\n",
    "                current_run = run\n",
    "                \n",
    "    features_to_plot = [\n",
    "        'init_lumi', \n",
    "        'pileup', \n",
    "        'recorded_lumi', \n",
    "        'delivered_lumi'\n",
    "    ]\n",
    "\n",
    "    filtered_lumi_info.plot(x='run_lumisection', y=features_to_plot, secondary_y='pileup', ax=ax, style=\"o\", markersize=1)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=6)\n",
    "    ax.set_yscale('log')\n",
    "    plt.title(f\"Run {run_begin} to {run_end}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b245e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lumis(315357, 315366, lumi_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bc8cf6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_lumis(lumi_info[\"run_number\"].min(), 315270, lumi_info, run_labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86250b61",
   "metadata": {},
   "source": [
    "# Reference Run Ranking Using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706cdc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off warnings for pandas\n",
    "pd.options.mode.chained_assignment = None\n",
    "importlib.reload(rrr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95822154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run level attributes compatible with PCA\n",
    "runfeaturelst = [\n",
    "    \"run_number\", #\n",
    "    \"init_lumi\", #\n",
    "    \"recorded_lumi\",\n",
    "    \"energy\",\n",
    "    \"end_lumi\", #\n",
    "    \"hlt_physics_rate\",\n",
    "#     \"fill_number\",\n",
    "#     \"initial_prescale_index\",\n",
    "#     \"last_lumisection_number\",\n",
    "    \"l1_rate\",\n",
    "    \"hlt_physics_counter\",\n",
    "]\n",
    "\n",
    "# Lumisecion (LS) level attributes compatible with PCA\n",
    "lsfeaturelst = [\n",
    "    \"run_number\",\n",
    "#     \"fill_number\",\n",
    "    \"lumisection_number\",\n",
    "    \"pileup\",\n",
    "#     \"delivered_lumi\",\n",
    "#     \"recorded_lumi\",\n",
    "    \"init_lumi\",\n",
    "    \"end_lumi\",\n",
    "]\n",
    "\n",
    "run_df = run_df[runfeaturelst]\n",
    "ls_df = ls_df[lsfeaturelst]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4297c4",
   "metadata": {},
   "source": [
    "## Additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd5c6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing change in total luminosity across the run\n",
    "run_df = rrr.comp_delta_totallumi(run_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b94d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical LS level features of interest\n",
    "lsfeaturelst = [\n",
    "    'run_number', \n",
    "    'lumisection_number',\n",
    "    'init_lumi', \n",
    "    'end_lumi', \n",
    "    'pileup'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6a2475",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_df.describe().loc[\"max\"][2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef4f292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the init_lumi, end_lumi and pileup std, mean, max and min over the run\n",
    "names = ['init_lumi', 'end_lumi', 'pileup']\n",
    "names = [name + suffix for suffix in ['_std', '_mean', \"_max\", \"_min\"] for name in names]\n",
    "\n",
    "lsstats_dict = {}\n",
    "for run in ls_df['run_number'].unique():\n",
    "    runlsstats = ls_df[ls_df['run_number'] == run].describe()\n",
    "    stds = runlsstats.loc['std'].to_list()[2:]\n",
    "    means = runlsstats.loc['mean'].to_list()[2:]\n",
    "    maxs = runlsstats.loc[\"max\"].to_list()[2:]\n",
    "    mins = runlsstats.loc[\"min\"].to_list()[2:]\n",
    "    lsstats_dict[run] = {name: stat for name, stat in zip(names, stds + means + maxs + mins)}    \n",
    "    \n",
    "# Constructing a lumisection dataframe to hold these features of interest.\n",
    "ls_features = pd.DataFrame(lsstats_dict).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228a912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.concat([run_df.set_index('run_number'), ls_features], axis=1).reset_index()\n",
    "features.rename(columns = {'index':'run_number'}, inplace=True)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5048f10f",
   "metadata": {},
   "source": [
    "## Weights & Correlations\n",
    "We can determine the weight each feature has relative to each other (if we interpret the coefficients of the first PC as a weight) as well as plot each feature against the others to get a sense of how correlated they all are.\n",
    "\n",
    "NOTE: `rrr.get_weights` needs to be run with matplotlib version <= 3.7 if using `plot=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0ea07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available features\")\n",
    "features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dc7fe8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the weights by taking the first principal component\n",
    "features_weights = rrr.get_weights(features, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f79e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting weights for each of the features\n",
    "ftrweights_dict = {feature: round(weight, 4) for feature, weight in zip(features.columns.to_list(), features_weights)}\n",
    "ftrweights_df = pd.DataFrame(list(ftrweights_dict.items()), columns=[\"Feature\", \"Weight\"]).sort_values(\"Weight\", ascending=False).reset_index(drop=True)\n",
    "ftrweights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ce2c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making correlation matrix\n",
    "corr_matrix = features.corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87035fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting correlation matrix\n",
    "plt.figure(figsize=(30, 30))\n",
    "# sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True, linewidth=0.5)\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", annot_kws={\"size\": 12})\n",
    "plt.title(\"Feature Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f0e3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting features with low correlation only\n",
    "# Higher threshold means we accept features that have higher correlation with eachother\n",
    "threshold = 0.6\n",
    "\n",
    "to_drop = set()\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i + 1, len(corr_matrix.columns)):\n",
    "        if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "            to_drop.add(corr_matrix.columns[j])\n",
    "to_drop = list(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd89174",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_lowcorr = features[features.columns[~features.columns.isin(to_drop)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262801bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_lowcorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6065356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting weights of only features that are not highly correlated\n",
    "featureslowcorr_weights = rrr.get_weights(features_lowcorr, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37454a79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking the weights for each of these low correlation features\n",
    "# Getting weights for each of the features\n",
    "ftrweightslowcorr_dict = {feature: round(weight, 4) for feature, weight in zip(features_lowcorr.columns.to_list(), featureslowcorr_weights)}\n",
    "ftrweightslowcorr_df = pd.DataFrame(list(ftrweightslowcorr_dict.items()), columns=[\"Feature\", \"Weight\"]).sort_values(\"Weight\", ascending=False).reset_index(drop=True)\n",
    "ftrweightslowcorr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3eba3c3",
   "metadata": {},
   "source": [
    "# Ranking With PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31de49e",
   "metadata": {},
   "source": [
    "## With `run_number`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78735d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting name of features that have high weights\n",
    "ftrs_to_use = ftrweightslowcorr_df[\"Feature\"][:6].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d56d78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only feaetures with high weight and low correlation between each other\n",
    "pcafeats = features_lowcorr[ftrs_to_use]\n",
    "pcafeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f79b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformatting df for use in ranking\n",
    "pcafeats[\"run\"] = pcafeats[\"run_number\"].astype(int)\n",
    "pcafeats.set_index(\"run\", inplace=True)\n",
    "pcafeats.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3f1a4b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initial test that ranking system actually ranks\n",
    "target = 316082\n",
    "targetRR = RRfetch(target, jsonfile='/eos/user/r/rcruzcan/SWAN_projects/RefRunRank/jsons/json_allRunsRefRuns.json')\n",
    "rankings = rrr.refrank_pca(pcafeats.loc[:target], target, n_components = 2)\n",
    "print('Ranking of RR used: ', rankings.index[rankings['run']==targetRR][0])\n",
    "rankings = pd.merge(rankings.set_index(\"run\"), pcafeats, left_index=True, right_index=True, how='left').reset_index()\n",
    "rankings.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b5fcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import trapz\n",
    "\n",
    "def test_ranking(features, comparison_num=60, n_components=1, print_stats=True, dpi=200):\n",
    "    # Testing over many target runs to obtain better idea of performance\n",
    "    RRranks = []\n",
    "    results = {}\n",
    "\n",
    "    # Loop over some of the runs that are available\n",
    "    for targ in list(features.reset_index()['run'].unique())[comparison_num+1:]:\n",
    "        # Get reference run\n",
    "        targetRR = RRfetch(targ, jsonfile='/eos/user/r/rcruzcan/SWAN_projects/RefRunRank/jsons/json_allRunsRefRuns.json')\n",
    "\n",
    "        rankings = rrr.refrank_pca(features.loc[:targ].iloc[-comparison_num:], targ, n_components=n_components)\n",
    "\n",
    "        # In case there are any target runs for which the number of candidate runs is 0\n",
    "        if rankings is None:\n",
    "            pass\n",
    "\n",
    "        # Get the rank the actual reference got\n",
    "        try: \n",
    "            actualrefrank = rankings.index[rankings['run']==targetRR][0]\n",
    "            RRranks.append(actualrefrank)\n",
    "            results[targ] = {'ActualRef': targetRR, 'ActualRefRank': actualrefrank}\n",
    "        except IndexError: # Error due to actual reference run not being available\n",
    "            continue\n",
    "\n",
    "\n",
    "    results = pd.DataFrame(results).T\n",
    "    \n",
    "    if print_stats:\n",
    "#         print(results['ActualRefRank'].describe())\n",
    "        # Lower mean rank indicates better performance\n",
    "        print(\"Mean rank of actual RR: {}\".format(np.mean(RRranks)))\n",
    "        # Less sensitive to outliers than mean rank, better idea of central tendency\n",
    "        print(\"Median rank of actual RR: {}\".format(np.median(RRranks)))\n",
    "        # Measures how often actual RR appears withing top-k ranks\n",
    "        print(\"Top-k accuracy (k=10): {}\".format(sum(rank < 10 for rank in RRranks) / len(RRranks)))\n",
    "        # Stat measure for evaluating processes that produce a list of possible responses to a sample of queries, ordered by probability of correctness. \n",
    "        # Its the average of the reciprocal ranks of results for a sample of queries\n",
    "        print(\"Mean reciprocal rank: {}\".format(np.mean([1.0 / (rank + 1) for rank in RRranks])))\n",
    "        \n",
    "        # Making CDF plot\n",
    "        RRranks_sorted = np.sort(RRranks)\n",
    "        \n",
    "        x_normalized = RRranks_sorted / RRranks_sorted.max()\n",
    "        cdf = np.arange(1, len(RRranks_sorted) + 1) / len(RRranks_sorted)\n",
    "        \n",
    "        auc = trapz(cdf, x_normalized)\n",
    "        \n",
    "        plt.plot(RRranks_sorted, cdf)\n",
    "        plt.xlabel(\"Rank of Actual RR\")\n",
    "        plt.ylabel(\"CDF\")\n",
    "        plt.title(\"CDF of Ranks of Actual RRs\")\n",
    "        plt.text(0.95, 0.05, f\"AUC (normalized): {auc:.2f}\", ha='right', va='bottom', transform=plt.gca().transAxes, fontsize=15, bbox=dict(facecolor='white', alpha=0.5))\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "\n",
    "    fig, ax = plt.subplots(dpi=dpi)\n",
    "    results['ActualRefRank'].hist(bins=30, ax=ax)\n",
    "    ax.set_title(\"RRR results for n={}, comparison_num={}\".format(n_components, comparison_num))\n",
    "    ax.set_xlabel(\"Rank of actual reference run\")\n",
    "\n",
    "    ax.axvline(x=10, color=\"r\", linestyle=\"--\", linewidth=2)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b708ee4b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Trying with temporal features\n",
    "for i in range(1, len(pcafeats.columns)):\n",
    "    test_ranking(pcafeats, n_components=i, print_stats=True, dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c2712b",
   "metadata": {},
   "source": [
    "## Without `run_number`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e43b8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting name of features that have high weights, but excluding run number\n",
    "feats_to_use = ftrweightslowcorr_df[\"Feature\"][:6].to_list()\n",
    "feats_to_use = [feat for feat in feats_to_use if feat != \"run_number\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a081144",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c809aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only features with high weight and low correlation between each other\n",
    "pcafeats_norunnum = features_lowcorr[feats_to_use + [\"run_number\"]] # Run number is kept just for formatting and id purposes\n",
    "pcafeats_norunnum.rename(columns={\"run_number\": \"run\"}, inplace=True)\n",
    "pcafeats_norunnum.set_index(\"run\", inplace=True)\n",
    "pcafeats_norunnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52300b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial test that ranking system actually ranks\n",
    "target = 316082\n",
    "targetRR = RRfetch(target, jsonfile='/eos/user/r/rcruzcan/SWAN_projects/RefRunRank/jsons/json_allRunsRefRuns.json')\n",
    "rankings = rrr.refrank_pca(pcafeats_norunnum.loc[:target], target, n_components = 2)\n",
    "print('Ranking of RR used: ', rankings.index[rankings['run']==targetRR][0])\n",
    "rankings = pd.merge(rankings.set_index(\"run\"), pcafeats_norunnum, left_index=True, right_index=True, how='left').reset_index()\n",
    "rankings.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607b3d22",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# More comprehensive performance testing\n",
    "for i in range(1, len(pcafeats_norunnum.columns)):\n",
    "    test_ranking(pcafeats_norunnum, n_components=i, print_stats=True, dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5930f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
