{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d708d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import importlib\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "sys.path.append(os.path.abspath('../utils/'))\n",
    "\n",
    "# local modules\n",
    "import get_oms_data\n",
    "# importlib.reload(get_oms_data)  # not needed unless making live changes to the modules\n",
    "from get_oms_data import get_oms_api, get_oms_data, get_oms_response_attribute\n",
    "\n",
    "import json_utils as jsonu\n",
    "import plot_utils as pu\n",
    "\n",
    "import mplhep as hep\n",
    "hep.style.use(\"CMS\")\n",
    "\n",
    "import OMS_RR_utils\n",
    "# importlib.reload(OMS_RR_utils) \n",
    "import json_utils\n",
    "import OMS_RR_utils as omsu\n",
    "from refruns_utils import get_reference_run as RRfetch\n",
    "import refrank_utils as rrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a0c37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(rrr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20465e66",
   "metadata": {},
   "source": [
    "# Getting the Data\n",
    "We start by instantiating the OMS API. The OMS API will be used to get Run and LS level data and will be the main source of information for the reference run ranking (RRR) system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5291df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "omsapi = get_oms_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1f79f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run level attributes\n",
    "run_attribs = [\n",
    "    'components', \n",
    "    'init_lumi',\n",
    "    'recorded_lumi', \n",
    "    'delivered_lumi',\n",
    "    'start_time',\n",
    "    'end_time',\n",
    "    'energy', \n",
    "    'end_lumi', \n",
    "    'hlt_physics_rate',\n",
    "    'fill_number', \n",
    "    'l1_hlt_mode', \n",
    "    'trigger_mode',\n",
    "    'l1_key_stripped', \n",
    "    'fill_type_party2',\n",
    "    'fill_type_party1', \n",
    "    'initial_prescale_index',\n",
    "    'sequence', \n",
    "    'hlt_physics_size', \n",
    "    'fill_type_runtime',\n",
    "    'last_lumisection_number', # refers to the last LSs where cms_active is True\n",
    "    'l1_rate',\n",
    "    'l1_menu', \n",
    "    'run_number',\n",
    "    'stable_beam',\n",
    "    'hlt_physics_counter',\n",
    "#     'peak_pileup' <-- Find actual name\n",
    "]\n",
    "\n",
    "# Lumisecion (LS) level attributes\n",
    "ls_attribs = [\n",
    "    'fill_number',\n",
    "    \"run_number\",\n",
    "    'lumisection_number',\n",
    "    \"physics_flag\",\n",
    "    \"cms_active\",\n",
    "    'bpix_ready',\n",
    "    'fpix_ready',\n",
    "    'tecm_ready',\n",
    "    'tecp_ready',\n",
    "    'tibtid_ready',\n",
    "    'tob_ready',\n",
    "    'pileup',\n",
    "    'delivered_lumi',\n",
    "    'recorded_lumi',\n",
    "    \"init_lumi\",\n",
    "    'end_lumi',\n",
    "    'beam1_stable',\n",
    "    'beam2_stable',\n",
    "    'beam2_present',\n",
    "    'beam1_present',\n",
    "#     \"l1_rate\" \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb1cb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run we wish to certify and for which we will find a correspond reference run\n",
    "target = 316201\n",
    "newest_run = 316201\n",
    "oldest_run = 314206 # -2000\n",
    "\n",
    "# Range of runs of interest\n",
    "run_range = (oldest_run, newest_run)\n",
    "print(run_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c51004",
   "metadata": {},
   "outputs": [],
   "source": [
    "target - oldest_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474babbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load run level data into JSON\n",
    "run_json = get_oms_data( # This function is good enough to use as is, no need to circumvent entry limit\n",
    "    omsapi, \n",
    "    'runs', \n",
    "    run_range, \n",
    "    limit_entries = 5_000,\n",
    "    attributes = run_attribs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5e7a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of runs: \", len(run_json['data']))\n",
    "print(\"Number of lumisections will be: ~\", 100 * len(run_json['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe7b44a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_df = omsu.makeDF(run_json).convert_dtypes()\n",
    "run_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3579b454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lumisecion level data into JSON\n",
    "ls_json = get_oms_data(\n",
    "    omsapi, \n",
    "    'lumisections', \n",
    "    run_range, \n",
    "    limit_entries=100_000,\n",
    "    attributes=ls_attribs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e8c217",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ls_json['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41507ea5",
   "metadata": {},
   "source": [
    "Loading data into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb633bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert JSON into DF\n",
    "run_df = omsu.makeDF(run_json).convert_dtypes()\n",
    "ls_df = omsu.makeDF(ls_json).convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f9f774",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3448b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f6251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save to parquet\n",
    "run_df.to_parquet('/eos/user/r/rcruzcan/SWAN_projects/RefRunRank/data/run_df.parquet')\n",
    "ls_df.to_parquet('/eos/user/r/rcruzcan/SWAN_projects/RefRunRank/data/ls_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d214fec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Load from parquet\n",
    "run_df = pd.read_parquet('/eos/user/r/rcruzcan/SWAN_projects/RefRunRank/data/run_df.parquet')\n",
    "ls_df = pd.read_parquet('/eos/user/r/rcruzcan/SWAN_projects/RefRunRank/data/ls_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2730c5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check of the loaded runs and LSs\n",
    "print(len(run_df))\n",
    "print(len(ls_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988f771c",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_df[[\n",
    "    \"run_number\", #\n",
    "    \"init_lumi\", #\n",
    "    \"recorded_lumi\",\n",
    "    \"energy\",\n",
    "    \"end_lumi\", #\n",
    "    \"hlt_physics_rate\",\n",
    "    \"fill_number\",\n",
    "    \"initial_prescale_index\",\n",
    "    \"last_lumisection_number\",\n",
    "    \"l1_rate\",\n",
    "    \"hlt_physics_counter\",\n",
    "]].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b9ca5a",
   "metadata": {},
   "source": [
    "Some of the rows has `fill_number = NaN`. This is problematic, so we will segment the rows into those that have this problem (`run_df_NaN`), and the rest (`run_df`). We also add `Fill Location` column and make the `run_number` column into an index column.\n",
    "\n",
    "<font color='red'> INVESTIGATE WHY THESE RUNS/LSs HAVE NaN VALUES. FIGURE WHERE THESE LSs TEND TO HAPPEN (START OR END OF THE RUN). </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b85fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out runs that have NaN in fill number\n",
    "run_df, run_df_NaN = omsu.has_fill(run_df)\n",
    "print(\n",
    "    '''\n",
    "    Runs kept in run dataframe: {}\n",
    "    Runs filtered out: {}\n",
    "    Lowest run # kept: {}\n",
    "    Highest run # kept: {}\n",
    "    '''.format(\n",
    "        len(run_df['run_number'].unique()), \n",
    "        len(run_df_NaN['run_number'].unique()),\n",
    "        run_df['run_number'].min(),\n",
    "        run_df['run_number'].max())\n",
    ")\n",
    "\n",
    "run_df = omsu.add_loc_wrt_fill(run_df)\n",
    "\n",
    "run_df.set_index(['run_number'], inplace=True)\n",
    "run_df.sort_index(level=['run_number'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de40d3bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ls_df, ls_df_NaN = omsu.has_fill(ls_df)\n",
    "print(\n",
    "    '''\n",
    "    Runs kept in lumisection dataframe: {}\n",
    "    Runs filtered out: {}\n",
    "    Lowest run # kept: {}\n",
    "    Highest run # kept: {}\n",
    "    '''.format(\n",
    "        len(ls_df['run_number'].unique()),   \n",
    "        len(ls_df_NaN['run_number'].unique()),\n",
    "        ls_df['run_number'].min(),\n",
    "        ls_df['run_number'].max()\n",
    "    )\n",
    ")\n",
    "\n",
    "ls_df = omsu.add_loc_wrt_fill(ls_df)\n",
    "ls_df = ls_df.convert_dtypes()\n",
    "\n",
    "ls_df.set_index(['run_number', 'lumisection_number'], inplace=True)\n",
    "ls_df.sort_index(level=['run_number', 'lumisection_number'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc17da2",
   "metadata": {},
   "source": [
    "We can now take a look at the data that was filtered out and the run data that is left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dcdd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581205c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71557daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking which runs appear in one dataframe, but not the other, and dropping them from both dataframes.\n",
    "\n",
    "rundf_runs = run_df.index.to_list()\n",
    "lsdf_runs = ls_df.index.get_level_values(0)\n",
    "\n",
    "# Finding symmetric different\n",
    "missing_runs = list(set(rundf_runs) ^ set(lsdf_runs))\n",
    "print(\n",
    "    '''\n",
    "    List of runs which are in one of the dataframes, but not the other (and viceversa): \\n    {}\n",
    "    '''.format(missing_runs)\n",
    ")\n",
    "\n",
    "# Going through each dataframe and deleting these runs\n",
    "for run in missing_runs:\n",
    "    if run in rundf_runs:\n",
    "        run_df.drop(run, inplace=True)\n",
    "    else:\n",
    "        ls_df.drop(run, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccada35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check that the run filtering step worked\n",
    "\n",
    "rundf_runs = run_df.index.to_list()\n",
    "lsdf_runs = ls_df.index.get_level_values(0)\n",
    "\n",
    "# Finding symmetric different\n",
    "missing_runs = list(set(rundf_runs) ^ set(lsdf_runs))\n",
    "print(\n",
    "    '''\n",
    "    List of runs which are in one of the dataframes, but not the other (and viceversa): \\n    {}\n",
    "    '''.format(missing_runs)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc6f4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    '''\n",
    "    Number of runs in the run dataframe: {}\n",
    "    Number of runs in the lumisection dataframe: {}\n",
    "    Are the runs contained in these dataframes exactly the same?: {}\n",
    "    '''.format(\n",
    "        len(run_df.index.unique()),\n",
    "        len(ls_df.index.get_level_values(0).unique()),\n",
    "        run_df.index.unique().tolist() == ls_df.index.get_level_values(0).unique().tolist()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ea5ad4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd343ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0df9c2",
   "metadata": {},
   "source": [
    "# Filtering Good & Collisions Runs/LSs\n",
    "\n",
    "Note: It might be neccesary to generate a new golden JSON in RR depending on the range of runs chosen for ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8925377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant golden jsons available in the json directory\n",
    "golden_files = [\n",
    "    'json_GOLDEN_2017.json', \n",
    "    'json_GOLDEN_2018.json',\n",
    "    'json_GOLDEN_RRRdev.json',\n",
    "    'json_TRK_PromptReco_314324to316201.json',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dc3fa1",
   "metadata": {},
   "source": [
    "Generated golden JSON such as `json_GOLDEN_RRRdev.json` made in RR using input similar to:\n",
    "```json\n",
    "{\n",
    "  \"and\": [\n",
    "    {\">=\": [{\"var\": \"run.oms.run_number\"}, 315190]},\n",
    "    {\"<=\": [{\"var\": \"run.oms.run_number\"}, 316201]},\n",
    "    {\"==\": [{\"var\": \"lumisection.rr.tracker-pixel\"}, \"GOOD\"]},\n",
    "    {\"==\": [{\"var\": \"lumisection.rr.tracker-strip\"}, \"GOOD\"]},\n",
    "    {\"==\": [{\"var\": \"lumisection.rr.tracker-track\"}, \"GOOD\"]}\n",
    "  ]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5c545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now get collision runs/LSs\n",
    "runcoll_df, lscoll_df, runnotcoll_df, lsnotcoll_df = omsu.get_collisions(run_df.reset_index(), ls_df.reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d81a4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    '''\n",
    "        We originally had this amount of runs:    {}\n",
    "        Amount of these runs that are collisions: {}\n",
    "    '''.format(len(runcoll_df['run_number'].tolist()) + len(runnotcoll_df['run_number'].tolist()),\n",
    "              len(runcoll_df['run_number'].unique()))\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6fc1f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    '''\n",
    "    Number of runs in collision dataframe:                   {}\n",
    "    Number of runs in LS dataframe:                          {}\n",
    "    Runs that are in one, but not the other (and viceversa): {}\n",
    "    '''.format(\n",
    "        len(runcoll_df['run_number'].unique()),\n",
    "        len(lscoll_df['run_number'].unique()),\n",
    "        list(set(runcoll_df['run_number'].tolist()) ^ set(lscoll_df['run_number'].unique().tolist()))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de28c9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dividing good runs/LSs from bad runs/LSs\n",
    "good_mask = json_utils.injson(\n",
    "    np.array(lscoll_df['run_number']), \n",
    "    np.array(lscoll_df['lumisection_number']), \n",
    "    '/eos/user/r/rcruzcan/SWAN_projects/RefRunRank/jsons/'+golden_files[3])\n",
    "\n",
    "lscollgood_df = lscoll_df[good_mask]\n",
    "lscollbad_df = lscoll_df[~good_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c423d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "runcollgood_df = runcoll_df[runcoll_df['run_number'].isin(lscollgood_df['run_number'].unique())]\n",
    "runcollbad_df = runcoll_df[~runcoll_df['run_number'].isin(lscollgood_df['run_number'].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1f44ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    '''\n",
    "        Number of good collision LSs: {}\n",
    "        Number of bad collisions LSs: {}\n",
    "    '''.format(len(lscollgood_df), len(lscollbad_df))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f47355",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    '''\n",
    "        Number of good collision LSs: {}\n",
    "        Number of bad collisions LSs: {}\n",
    "    '''.format(len(runcollgood_df), len(runcollbad_df))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776281fd",
   "metadata": {},
   "source": [
    "<h3>Data Exploration</h3>\n",
    "\n",
    "We create some plots to better understand how this data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e2e1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lumi_info = lscollgood_df[['run_number',\n",
    "                           'init_lumi', \n",
    "                           'end_lumi',\n",
    "                           'pileup',\n",
    "                           'recorded_lumi', \n",
    "                           'delivered_lumi',\n",
    "                           'lumisection_number',]]\n",
    "lumi_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3920dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_begin = 315357\n",
    "run_end = 315366\n",
    "\n",
    "lumi_info['run_lumisection'] = list(zip(lumi_info['run_number'], lumi_info['lumisection_number']))\n",
    "\n",
    "filtered_lumi_info = lumi_info[(lumi_info['run_number'] >= run_begin) & (lumi_info['run_number'] <= run_end)]\n",
    "\n",
    "fig, ax = plt.subplots(dpi=200)\n",
    "\n",
    "current_run = None\n",
    "for idx, (run, lumisection) in enumerate(filtered_lumi_info['run_lumisection']):\n",
    "    if run != current_run:\n",
    "        ax.axvline(x=idx, color='gray', linestyle='--', linewidth=0.5)  # Add a vertical line\n",
    "        \n",
    "        # Add a label for the run number\n",
    "        if current_run is not None:  # Skip label for the very first line\n",
    "            label_x_position = idx - 1  # Adjust this as needed to position the label correctly\n",
    "            ax.text(label_x_position, 0.95, str(current_run), transform=ax.get_xaxis_transform(), \n",
    "                    horizontalalignment='right', verticalalignment='top', fontsize=6, color='gray', rotation='vertical')\n",
    "            \n",
    "        current_run = run\n",
    "        \n",
    "        \n",
    "features_to_plot = [\n",
    "    'init_lumi', \n",
    "    'pileup', \n",
    "    'recorded_lumi', \n",
    "    'delivered_lumi'\n",
    "]\n",
    "\n",
    "filtered_lumi_info.plot(x='run_lumisection', y=features_to_plot, secondary_y='pileup', ax=ax)\n",
    "ax.tick_params(axis='both', which='major', labelsize=6)\n",
    "ax.set_yscale('log')\n",
    "plt.title(f\"Run {run_begin} to {run_end}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c848b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "runcollgood_df[runcollgood_df['run_number']==315357]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e093bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "lscollgood_df[lscollgood_df['run_number']==315357].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d1039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lumi_info[(lumi_info['run_number'] >= run_begin) & (lumi_info['run_number'] <= run_end)].set_index('run_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0452386c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# lscoll_df[lscoll_df['run_number']].plot(y=['recorded_lumi','delivered_lumi','pileup'],style='-',figsize=(20,9),logy=False,logx=False,secondary_y=\"pileup\")\n",
    "lscollgood_df[lscollgood_df['run_number'] < 315270].plot(y=['recorded_lumi','delivered_lumi','pileup'],style='-',figsize=(20,9),logy=False,logx=False,secondary_y=\"pileup\")\n",
    "# plt.xlim((0,3800))\n",
    "plt.show()\n",
    "# lsdf_collisions.plot(y=['init_lumi','end_lumi'],x='run_number' ,figsize=(19,9),logy=False,logx=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7352d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lscollgood_df['run_number'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87bcbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=150)\n",
    "lscollgood_df[lscollgood_df['run_number'] <= 315264].reset_index().plot(y=['recorded_lumi', 'delivered_lumi', 'pileup'], ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85745ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lscollgood_df[lscollgood_df['run_number'] == 315259][['recorded_lumi', 'delivered_lumi', 'pileup']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ded728",
   "metadata": {},
   "outputs": [],
   "source": [
    "lscollgood_df[lscollgood_df['run_number'] <= 315265].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3292fc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=200)\n",
    "\n",
    "runcollgood_df.plot(y=['recorded_lumi',\"delivered_lumi\"], x=\"run_number\",\n",
    "                kind=\"bar\", figsize=(19,9), logy=False, logx=False, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4843f4d7",
   "metadata": {},
   "source": [
    "# Previously Developed Ranking Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c9fa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b238524",
   "metadata": {},
   "source": [
    "<h2> Pre-existing RRR systems </h2>\n",
    "\n",
    "The implementations previously developed used a dictionary as input where the elements of the dictionary were the run and LS dataframes. Therefore, we create such a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439116c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the dictionary out of dataframes\n",
    "data_dict = {'runs': runcollgood_df.reset_index(), 'lumisections': lscollgood_df.reset_index()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae29eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run we wish to certify and for which we will find a correspond reference run\n",
    "# target = 316201\n",
    "# oldest_run = target - 1011\n",
    "\n",
    "# # Range of runs of interest\n",
    "# run_range = (oldest_run, target)\n",
    "# print(run_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f586c3",
   "metadata": {},
   "source": [
    "We also fetch the actual RR used for the certification of the target run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578352a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "targetRR = RRfetch(target, jsonfile='/eos/user/r/rcruzcan/SWAN_projects/RefRunRank/utils/json_allRunsRefRuns.json')\n",
    "print('Actual RR used: ' + str(targetRR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e888bd9e",
   "metadata": {},
   "source": [
    "<h3> Version 1 </h3>\n",
    "Version 1 of the RRR system gives a rank to each of the runs given by the following equation. (Note: In the original proposed version 1, the equation is slighly altered. The equation shown here is the one found in the actual implementation.)\n",
    "\n",
    "$$\n",
    "    G_1 = 0.5 *(\\text{inst lumi delta }\\%)  + 0.25 *(\\text{pileup delta }\\%) + 0.25 *(\\text{run number delta } \\%)\n",
    "$$\n",
    "\n",
    "where\n",
    "- $\\text{quantity %} = \\frac{\\text{possible ref quantity}-\\text{target quantity}}{\\text{target quantity}} * 100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba260279",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "v1_ranking = omsu.ref_rank(data_dict, target, Trim=False, ver='V1').set_index('run_number') \\\n",
    "    .sort_values(by=[\"Run_Rank\",\"inst_lumi_delta %\",'pileup_delta %','run_number_delta'], key=lambda x: x.abs(), ascending=True)\n",
    "v1_ranking.reset_index(inplace=True)\n",
    "v1_ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1340f97",
   "metadata": {},
   "source": [
    "We now evaluate how well this ranking system is by checking which run was actually used as reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a44fd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ranking of actual RR used for target:', list(np.where(v1_ranking['run_number'] == targetRR))[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b0604c",
   "metadata": {},
   "source": [
    "<h3> Version 2 </h3>\n",
    "The equation used in this version to compute the ranking is given by\n",
    "$$\n",
    "    G_2 = 0.5 * \\frac{\\text{(inst lumi %)} * \\text{(run inst lumi)}}{100 * \\text{(ave inst lumi)}} + 0.25 * \\frac{\\text{(pileup %)}}{\\text{(run pileup)}} + 0.25 * \\frac{\\text{(run number %)}}{\\text{(run num)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6067958",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "v2_ranking = omsu.ref_rank(data_dict, target, Trim=False, ver='V2').set_index('run_number') \\\n",
    "    .sort_values(by=[\"Run_Rank\",\"inst_lumi_delta %\",'pileup_delta %','run_number_delta'], key=lambda x: x.abs(), ascending=True)\n",
    "v2_ranking.reset_index(inplace=True)\n",
    "v2_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5749335",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ranking of actual RR used for target:', list(np.where(v2_ranking['run_number'] == targetRR))[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da49add7",
   "metadata": {},
   "source": [
    "<h3> Version 3 </h3>\n",
    "The equation used for this version is given by\n",
    "$$\n",
    "    G_3 = \\frac{\\text{inst lumi delta %}}{100} + \\frac{\\text{pileup delta %}}{100} + \\frac{\\text{run num delta}}{100} + \\frac{\\text{num of lumi delta %}}{100}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2618a705",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "v3_ranking = omsu.ref_rank(data_dict, target, Trim=False, ver='V3').set_index('run_number') \\\n",
    "    .sort_values(by=[\"Run_Rank\",\"inst_lumi_delta %\",'pileup_delta %','run_number_delta'], key=lambda x: x.abs(), ascending=True)\n",
    "v3_ranking.reset_index(inplace=True)\n",
    "v3_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c61d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ranking of actual RR used for target:', list(np.where(v3_ranking['run_number'] == targetRR))[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333d8baa",
   "metadata": {},
   "source": [
    "# Reference Run Ranking Using PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfbd08f",
   "metadata": {},
   "source": [
    "We first take a observe the features available for PCA, find how they are correlated and their weights (given by the coefficients in the first PC), and then try using PCA to rank runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0ea07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Run level features: \\n{}\\n\".format(runcollgood_df.iloc[0]))\n",
    "print(\"LS level features: \\n{}\".format(lscollgood_df.iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d277fa4",
   "metadata": {},
   "source": [
    "## Run Level Features\n",
    "PCA compatible features that can be considered are\n",
    "- `run_number`\n",
    "- `init_lumi`\n",
    "- `recorded_lumi`\n",
    "- `energy`\n",
    "- `end_lumi`\n",
    "- `hlt_physics_rate`\n",
    "- `fill_number`\n",
    "- `initial_prescale_index`\n",
    "- `last_lumisection_number`\n",
    "- `l1_rate`\n",
    "- `hlt_physics_counter`\n",
    "\n",
    "Features that we could calculate\n",
    "- `delta_lumi` (change in lumi)\n",
    "- `temp_dist` (how far back in time a run is with respect to the target run)\n",
    "\n",
    "In this section we study these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32296f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "runcollgood_df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3372fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting total change in lumi\n",
    "runcollgood_df = rrr.comp_delta_totallumi(runcollgood_df)\n",
    "# Getting temporal distance of each run with respect to the target run\n",
    "runcollgood_df = rrr.comp_temp_dist(runcollgood_df, 316062) # target run chosen as example\n",
    "# Getting duration\n",
    "runcollgood_df = rrr.comp_duration(runcollgood_df)\n",
    "runcollgood_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1883c3e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "runcollgood_df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c763acd",
   "metadata": {},
   "source": [
    "We will now see how all of these variables are correlated and will also determine the weights of each of them relative to each other using the first component of PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3e8a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run level features that are compatible with PCA\n",
    "runfeaturelst = [\n",
    "    \"run_number\", #\n",
    "    \"init_lumi\", #\n",
    "    \"recorded_lumi\",\n",
    "    \"energy\",\n",
    "    \"end_lumi\", #\n",
    "    \"hlt_physics_rate\",\n",
    "    \"fill_number\",\n",
    "    \"initial_prescale_index\",\n",
    "    \"last_lumisection_number\",\n",
    "    \"l1_rate\",\n",
    "    \"hlt_physics_counter\",\n",
    "    \"delta_totallumi\", #\n",
    "    \"temp_dist\", #\n",
    "    \"delivered_lumi\" #\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d0d7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_features = runcollgood_df[runfeaturelst].astype(float).fillna(0)\n",
    "run_features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dc7fe8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the weights by taking the first principal component\n",
    "weights = rrr.get_weights(run_features, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bac0c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organizing features by order of imporant\n",
    "featureweights_dict = {feature: round(weight, 4) for feature, weight in zip(runfeaturelst, weights)}\n",
    "featureweights_df = pd.DataFrame(list(feature_weight_dict.items()), columns=[\"Feature\", \"Weight\"]).sort_values(\"Weight\", ascending=False).reset_index(drop=True)\n",
    "featureweights_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b199ab",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- `temp_dist`, `run_number` and `fill_number` are highly correlated. \n",
    "- `recorded_lumi` and `recorded_lumi` are highly correlated\n",
    "- `energy`, `l1_rate`, `hlt_physics_counter` are very unimportant\n",
    "- `last_lumisection_number` just tells us how long a run is. This is not relevant at this stage, so it is ignored for ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ffa5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking only at relevant and non-highly correlated features\n",
    "reducedfeaturelist = [\"init_lumi\", \"end_lumi\", \"delta_totallumi\", \"hlt_physics_rate\"]\n",
    "weights_reduced = rrr.get_weights(run_features[reduced_feature_list], plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d887ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organizing features by order of imporant\n",
    "featureweightsreduced_dict = {feature: round(weight, 4) for feature, weight in zip(reducedfeaturelist, weights_reduced)}\n",
    "featureweightsreduced_df = pd.DataFrame(list(featureweightsreduced_dict.items()), columns=[\"Feature\", \"Weight\"]).sort_values(\"Weight\", ascending=False).reset_index(drop=True)\n",
    "featureweightsreduced_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e026d440",
   "metadata": {},
   "source": [
    "## LS Level Features\n",
    "\n",
    "Features of interest from LS data:\n",
    "- average and std of `init_lumi`\n",
    "- average and std of `end_lumi`\n",
    "- average and std of `pile_up`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b94d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical LS level features of interest\n",
    "lsfeaturelst = [\n",
    "    'run_number', \n",
    "    'lumisection_number',\n",
    "    'init_lumi', \n",
    "    'end_lumi', \n",
    "    'pileup'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4a342f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lss = lscollgood_df[lsfeaturelst]\n",
    "lss.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef4f292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the init_lumi, end_lumi and pileup std and mean over the run\n",
    "names = ['init_lumi', 'end_lumi', 'pileup']\n",
    "names = [name + suffix for suffix in ['_std', '_mean'] for name in names ]\n",
    "\n",
    "lsstats_dict = {}\n",
    "for run in lss['run_number'].unique():\n",
    "    runlsstats = lss[lss['run_number']==run].describe()\n",
    "    stds = runlsstats.loc['std'].to_list()[2:]\n",
    "    means = runlsstats.loc['mean'].to_list()[2:]\n",
    "    lsstats_dict[run] = {name: stat for name, stat in zip(names, stds+means)}    \n",
    "    \n",
    "# Constructing a lumisection dataframe to hold these features of interest.\n",
    "ls_features = pd.DataFrame(lsstats_dict).T\n",
    "ls_features.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303452b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding weights for these features\n",
    "lsweights = rrr.get_weights(ls_features, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddd59a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organizing features by order of imporance\n",
    "lsfeaturelst = ls_features.columns.to_list()\n",
    "\n",
    "lsfeatureweights_dict = {feature: round(weight, 4) for feature, weight in zip(lsfeaturelst, weights)}\n",
    "lsfeatureweights_df = pd.DataFrame(list(lsfeatureweights_dict.items()), columns=[\"Feature\", \"Weight\"]).sort_values(\"Weight\", ascending=False).reset_index(drop=True)\n",
    "lsfeatureweights_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d79cffd",
   "metadata": {},
   "source": [
    "## Run + LS Features (Not finished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0e5e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.concat([run_features.set_index('run_number'), ls_features], axis=1).reset_index()\n",
    "features.rename(columns = {'index':'run_number'}, inplace=True)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0261eb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556910b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights3 = get_weights(features)\n",
    "print('Fevel feature weights: ')\n",
    "{feature: weight for feature, weight in zip(features.columns.to_list(), weights3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1a0242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without run number and temporal distance\n",
    "weights3 = get_weights(features[[\n",
    "    'init_lumi', \n",
    "    'end_lumi', \n",
    "    'delta_totallumi', \n",
    "    'delivered_lumi', \n",
    "    'init_lumi_std', \n",
    "    'end_lumi_std',\n",
    "    'pileup_std',\n",
    "    'init_lumi_mean',\n",
    "    'end_lumi_mean',\n",
    "    'pileup_mean'\n",
    "]])\n",
    "print('Feature weights: ')\n",
    "{feature: weight for feature, weight in zip(features[[\n",
    "    'init_lumi', \n",
    "    'end_lumi', \n",
    "    'delta_totallumi', \n",
    "    'delivered_lumi', \n",
    "    'init_lumi_std', \n",
    "    'end_lumi_std',\n",
    "    'pileup_std',\n",
    "    'init_lumi_mean',\n",
    "    'end_lumi_mean',\n",
    "    'pileup_mean'\n",
    "]].columns.to_list(), weights3)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35675727",
   "metadata": {},
   "source": [
    "- Features of importance after testing different combinations:\n",
    "    - `run_number`\n",
    "    - `temp_dist`\n",
    "    - `init_lumi`\n",
    "    - `end_lumi`\n",
    "    - `init_lumi_mean`\n",
    "    - `end_lumi_mean`\n",
    "- Including features related to the standard deviation of a LS-lel quantity worsened the performance of the ranking considerably.\n",
    "- Including `pileup_mean`, `delta_totallumi` and/or `delivered_lumi` make the ranking performance worse, but not by much if only one of these are included at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd6034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_use = ['run_number', 'temp_dist', 'init_lumi', 'end_lumi', 'init_lumi_mean', 'end_lumi_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f9c974",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.set_index('run_number').loc[:315267]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3eba3c3",
   "metadata": {},
   "source": [
    "# Ranking With PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db2d1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(rrr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d56d78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features dataframe\n",
    "features = pd.concat([run_features.set_index('run_number'), ls_features], axis=1).reset_index()\n",
    "features.rename(columns = {'index':'run_number'}, inplace=True)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b490c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features that will be considered in the ranking\n",
    "features_to_use = [\n",
    "    'run_number',\n",
    "    'init_lumi',\n",
    "#     'recorded_lumi',\n",
    "#     'energy',\n",
    "    'end_lumi',\n",
    "#     'hlt_physics_rate',\n",
    "#     'fill_number',\n",
    "#     'initial_prescale_index',\n",
    "#     'last_lumisection_number',\n",
    "#     'l1_rate',\n",
    "#     'hlt_physics_counter',\n",
    "    'delta_totallumi',\n",
    "#     'temp_dist',\n",
    "    'delivered_lumi',\n",
    "    'init_lumi_std',\n",
    "    'end_lumi_std',\n",
    "    'pileup_std',\n",
    "    'init_lumi_mean',\n",
    "    'end_lumi_mean',\n",
    "    'pileup_mean'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f79b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformatting df for use in ranking\n",
    "features['run'] = features['run_number'].astype(int)\n",
    "features.set_index('run', inplace=True)\n",
    "features.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3f1a4b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initial test that ranking system actually ranks\n",
    "target = 316082\n",
    "targetRR = RRfetch(target, jsonfile='/eos/user/r/rcruzcan/SWAN_projects/RefRunRank/jsons/json_allRunsRefRuns.json')\n",
    "rankings = rrr.refrank_pca(features[features_to_use].loc[:target], target, n_components=2)\n",
    "print('Ranking of RR used: ', rankings.index[rankings['run']==targetRR][0])\n",
    "rankings = pd.merge(rankings.set_index(\"run\"), features[features_to_use], left_index=True, right_index=True, how='left').reset_index()\n",
    "rankings.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d86c02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing a case where there will not be enough candidate runs to apply ranking\n",
    "target = 314472 # Oldest run in features dataframe, so nothing to compare it to\n",
    "targetRR = RRfetch(target, jsonfile='/eos/user/r/rcruzcan/SWAN_projects/RefRunRank/jsons/json_allRunsRefRuns.json')\n",
    "rankings = rrr.refrank_pca(features[features_to_use], target, n_components=1)\n",
    "rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b5fcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ranking(features, comparison_num=60, n_components=1, print_stats=True, dpi=200):\n",
    "    # Testing over many target runs to obtain better idea of performance\n",
    "    RRranks = []\n",
    "    results = {}\n",
    "\n",
    "    # Loop over some of the runs that are available\n",
    "    for targ in list(features.reset_index()['run'].unique())[comparison_num+1:]:\n",
    "        # Get reference run\n",
    "        targetRR = RRfetch(targ, jsonfile='/eos/user/r/rcruzcan/SWAN_projects/RefRunRank/jsons/json_allRunsRefRuns.json')\n",
    "\n",
    "        rankings = rrr.refrank_pca(features[features_to_use].loc[:targ].iloc[-comparison_num:], targ, n_components=n_components)\n",
    "\n",
    "        # In case there are any target runs for which the number of candidate runs is 0\n",
    "        if rankings is None:\n",
    "            pass\n",
    "\n",
    "        # Get the rank the actual reference got\n",
    "        try: \n",
    "            actualrefrank = rankings.index[rankings['run']==targetRR][0]\n",
    "            RRranks.append(actualrefrank)\n",
    "            results[targ] = {'ActualRef': targetRR, 'ActualRefRank': actualrefrank}\n",
    "        except: # Error due to actual reference run not being available\n",
    "            pass\n",
    "\n",
    "\n",
    "    results = pd.DataFrame(results).T\n",
    "    \n",
    "    if print_stats:\n",
    "        print(results['ActualRefRank'].describe())\n",
    "\n",
    "    fig, ax = plt.subplots(dpi=dpi)\n",
    "    ax = results['ActualRefRank'].hist(bins=30)\n",
    "    ax.set_title(\"RRR results for n={}, comparison_num={}\".format(n_components, comparison_num))\n",
    "    ax.set_xlabel(\"Rank of actual reference run\")\n",
    "\n",
    "    ax.plot([10]*10, range(0,10))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ad20f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features that will be considered in the ranking\n",
    "features_to_use = [\n",
    "    'run_number',\n",
    "    'init_lumi',\n",
    "#     'recorded_lumi',\n",
    "#     'energy',\n",
    "    'end_lumi',\n",
    "#     'hlt_physics_rate',\n",
    "#     'fill_number',\n",
    "#     'initial_prescale_index',\n",
    "#     'last_lumisection_number',\n",
    "#     'l1_rate',\n",
    "#     'hlt_physics_counter',\n",
    "    'delta_totallumi',\n",
    "    'temp_dist',\n",
    "    'delivered_lumi',\n",
    "    'init_lumi_std',\n",
    "    'end_lumi_std',\n",
    "    'pileup_std',\n",
    "    'init_lumi_mean',\n",
    "    'end_lumi_mean',\n",
    "    'pileup_mean'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b708ee4b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Trying with temporal features\n",
    "for i in range(1, 5):\n",
    "    test_ranking(features[features_to_use], n_components=i, print_stats=True, dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6963ca3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff867fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21086226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0cf0cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ae393a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7120a8fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0136a006",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cecc9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-testing, but excluding runs that have low number of LSs\n",
    "isrunlong = {}\n",
    "for run in lss['run_number'].unique():\n",
    "    run_length = len(lss[lss['run_number']==run])\n",
    "    isrunlong[run] = run_length > 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f673cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_longruns = features.set_index('run_number')[pd.Series(isrunlong)].reset_index()\n",
    "features_longruns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983a49bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing over many target runs to obtain better idea of performance\n",
    "RRranks = []\n",
    "results = {}\n",
    "\n",
    "# Loop over some of the runs that are available\n",
    "for targ in list(features_longruns.reset_index()['run'].unique())[34:]:\n",
    "    # Get reference run\n",
    "    targetRR = RRfetch(targ, jsonfile='/eos/user/r/rcruzcan/SWAN_projects/RefRunRank/utils/json_allRunsRefRuns.json')\n",
    "    \n",
    "    rankings = rrr.refrank_pca(features_longruns[features_to_use], targ, n_components=1)\n",
    "\n",
    "    # In case there are any target runs for which the number of candidate runs is 0\n",
    "    if rankings is None:\n",
    "        pass\n",
    "    \n",
    "    # Get the rank the actual reference got\n",
    "    actualrefrank = rankings.index[rankings['run']==targetRR][0]\n",
    "    RRranks.append(actualrefrank)\n",
    "    results[targ] = {'ActualRef': targetRR, 'ActualRefRank': actualrefrank}\n",
    "\n",
    "results = pd.DataFrame(results).T\n",
    "print(results['ActualRefRank'].describe())\n",
    "\n",
    "ranksyseff = len(results[results['ActualRefRank'] <= 10])/len(results)\n",
    "print('Rank system efficiency: ', ranksyseff)\n",
    "\n",
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax = results['ActualRefRank'].hist(bins=30)\n",
    "\n",
    "ax.plot([10]*10, range(0,10))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# # for i in range(len(features_to_use)):\n",
    "# rankings = []\n",
    "# results = {}\n",
    "\n",
    "# for targ in list(features_longruns['run_number'].unique()):\n",
    "#     targetRR = RRfetch(targ, jsonfile='/eos/user/r/rcruzcan/SWAN_projects/RefRunRank/utils/json_allRunsRefRuns.json')\n",
    "#     features_PC = rrr.refrank_pca(features_longruns[features_to_use], targ, n_components=2)\n",
    "#     actualrefrank = features_PC.index[features_PC['run_number']==targetRR]\n",
    "#     if len(actualrefrank) > 0:\n",
    "#         rankings.append(actualrefrank[0])\n",
    "#         results[targ] = {'ActualRef': targetRR, 'ActualRefRank': actualrefrank[0]}\n",
    "    \n",
    "# results = pd.DataFrame(results).T\n",
    "# print(results['ActualRefRank'].describe())\n",
    "\n",
    "# ranksyseff = len(results[results['ActualRefRank'] <= 10])/len(results)\n",
    "# print('Rank system efficiency: ', ranksyseff)\n",
    "\n",
    "# fig, ax = plt.subplots(dpi=150)\n",
    "# ax = results['ActualRefRank'].hist(bins=30)\n",
    "\n",
    "# ax.plot([10]*10, range(0,10))\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a9fb2c",
   "metadata": {},
   "source": [
    "Trying with n > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2511399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing\n",
    "scaler = StandardScaler()\n",
    "features_scaled = pd.DataFrame(scaler.fit_transform(features[features_to_use]), columns=features[features_to_use].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559194c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PCA\n",
    "# n_components = 1\n",
    "# pca = PCA(n_components=n_components)\n",
    "# pca.fit(features_scaled)\n",
    "\n",
    "# features_PC = pd.DataFrame(pca.transform(features_scaled), columns=['PC'+str(i+1) for i in range(len(pca.components_))])\n",
    "# features_PC = pd.concat([features['run_number'], features_PC], axis=1).set_index('run_number')\n",
    "\n",
    "# # Getting distances\n",
    "# dist = np.sqrt(((features_PC - features_PC.loc[target])**2).sum(axis=1))\n",
    "# features_PC = pd.concat([dist, features_PC], axis=1)\n",
    "# features_PC.rename(columns = {0:'dist'}, inplace=True)\n",
    "\n",
    "# # Sorting by distance\n",
    "# features_PC = features_PC.sort_values(by='dist', ascending=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28e4f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Ranking of RR used: ', features_PC.index[features_PC['run_number']==targetRR][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae34147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA n=1\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(features_scaled)\n",
    "\n",
    "# Projecting data to sub-space\n",
    "features_red1 = pd.DataFrame(pca.transform(features_scaled), columns=['PC1'])\n",
    "features_red1 = pd.concat([features['run_number'], features_red1], axis=1).set_index('run_number')\n",
    "\n",
    "# Getting distances\n",
    "dist_n1 = (features_red1 - features_red1.loc[target]).abs()\n",
    "dist_n1.rename(columns = {'PC1':'dist'}, inplace=True)\n",
    "features_red1 = pd.concat([dist_n1, features_red1], axis=1)\n",
    "\n",
    "# Sorting by distance\n",
    "features_red1 = features_red1.sort_values(by='dist', ascending=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceec7da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_red1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f69877a",
   "metadata": {},
   "source": [
    "With the rankings (i.e. index of `feature_red1`) for all the runs under consideration, we can get the rank of the reference run that was actually used, which is contained in `targetRR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6c6d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ranking of RR used: ', features_red1.index[features_red1['run_number']==targetRR][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cd7e4e",
   "metadata": {},
   "source": [
    "We now try with $N=2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2a263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA n=2\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(features_scaled)\n",
    "\n",
    "# Projecting data to sub-space\n",
    "features_red2 = pd.DataFrame(pca.transform(features_scaled), columns=['PC1', 'PC2'])\n",
    "features_red2 = pd.concat([features['run_number'], features_red2], axis=1).set_index('run_number')\n",
    "\n",
    "# Computing Eucledian distances\n",
    "dist_n2 = np.sqrt(((features_red2 - features_red2.loc[target])**2).sum(axis=1))\n",
    "features_red2 = pd.concat([dist_n2, features_red2], axis=1)\n",
    "features_red2.rename(columns = {0:'dist'}, inplace=True)\n",
    "features_red2 = features_red2.sort_values(by='dist', ascending=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a93e1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_red2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21246f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ranking of RR used: ', features_red2.index[features_red2['run_number']==targetRR][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e566ca4",
   "metadata": {},
   "source": [
    "Trying $N=3$..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf994f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA n=3\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(features_scaled)\n",
    "\n",
    "# Projecting data to sub-space\n",
    "features_red3 = pd.DataFrame(pca.transform(features_scaled), columns=['PC1', 'PC2', 'PC3'])\n",
    "features_red3 = pd.concat([features['run_number'], features_red3], axis=1).set_index('run_number')\n",
    "\n",
    "# Computing Eucledian distances\n",
    "dist_n3 = np.sqrt(((features_red3 - features_red3.loc[target])**2).sum(axis=1))\n",
    "features_red3 = pd.concat([dist_n3, features_red3], axis=1)\n",
    "features_red3.rename(columns = {0:'dist'}, inplace=True)\n",
    "features_red3 = features_red3.sort_values(by='dist', ascending=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982ee846",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_red3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626870f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ranking of RR used: ', features_red3.index[features_red3['run_number']==targetRR][0])"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
